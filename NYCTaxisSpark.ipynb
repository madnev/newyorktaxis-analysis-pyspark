{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3d64f0fd-710e-48c7-8e5b-e56c877e3f79",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Advanced Data Analysis Project: NYC Taxi Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "51823ee2-b2f5-4933-b3a3-3f52853b5339",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Useful References:\n",
    "* [ACM DEBS 2015 Grand Challenge](http://www.debs2015.org/call-grand-challenge.html)\n",
    "* [Spark web site](https://spark.apache.org/)\n",
    "* [Spark MLlib main page](https://spark.apache.org/mllib/)\n",
    "* [Spark MLlib guide](https://spark.apache.org/docs/latest/ml-guide.html)\n",
    "* [Spark GraphX main page](https://spark.apache.org/graphx/)\n",
    "* [Spark GraphFrames main page](https://graphframes.github.io/graphframes/docs/_site/index.html)\n",
    "* [Spark GraphFrames User Guide](https://graphframes.github.io/graphframes/docs/_site/user-guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0d890751-3fdd-4788-a25d-d9bb16bdf68e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "use_sample = True\n",
    "\n",
    "if 'DATABRICKS_RUNTIME_VERSION' in os.environ : \n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"Group project\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    spark.conf.set(\"fs.azure.sas.data.novasbeadatrab.blob.core.windows.net\",\n",
    "      \"https://novasbeadatrab.blob.core.windows.net/?sv=2019-12-12&ss=b&srt=co&sp=rl&se=2020-12-19T06:59:10Z&st=2020-11-22T22:59:10Z&spr=https&sig=jMOoA0U33yOje%2F5mRkcJYuIEbM6K3i02zGKk4p%2BGXkc%3D\")\n",
    "    if use_sample:\n",
    "        FILENAME = \"wasbs://data@novasbeadatrab.blob.core.windows.net/sample.csv\"\n",
    "    else:\n",
    "        FILENAME = \"wasbs://data@novasbeadatrab.blob.core.windows.net/sorted_data.csv\"    \n",
    "else:\n",
    "    if use_sample:\n",
    "        FILENAME = \"data/sample.csv\"\n",
    "    else:\n",
    "        FILENAME = \"data/sorted_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d1a78a0d-54dd-4171-afe4-2427474f781e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-----\n",
    "## Simple statistics\n",
    "\n",
    "First program prints simple statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Group project\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4b8683ef-8e47-4517-b023-9417fdcd5d85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Group project\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "mySchema = StructType([\n",
    "    StructField(\"medallion\", StringType()),\n",
    "    StructField(\"hack_license\", StringType()),\n",
    "    StructField(\"pickup_datetime\", TimestampType()),\n",
    "    StructField(\"dropoff_datetime\", TimestampType()),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType()),\n",
    "    StructField(\"trip_distance\", DoubleType()),\n",
    "    StructField(\"pickup_longitude\", DoubleType()),\n",
    "    StructField(\"pickup_latitude\", DoubleType()),\n",
    "    StructField(\"dropoff_longitude\", DoubleType()),\n",
    "    StructField(\"dropoff_latitude\", DoubleType()),\n",
    "    StructField(\"payment_type\", StringType()),\n",
    "    StructField(\"fare_amount\", DoubleType()),\n",
    "    StructField(\"surcharge\", DoubleType()),\n",
    "    StructField(\"mta_tax\", DoubleType()),\n",
    "    StructField(\"tip_amount\", DoubleType()),\n",
    "    StructField(\"tolls_amount\", DoubleType()),\n",
    "    StructField(\"total_amount\", DoubleType()),\n",
    "])\n",
    "\n",
    "dataset = spark.read.load(FILENAME, format=\"csv\", \n",
    "                         sep=\",\", schema=mySchema, header=\"false\")\n",
    "dataset.createOrReplaceTempView(\"data\")\n",
    "\n",
    "statistics = spark.sql( \"\"\"SELECT COUNT( DISTINCT medallion) AS num_medallion, \n",
    "                                  COUNT( DISTINCT hack_license) AS num_license,\n",
    "                                  MIN( pickup_datetime) AS min_pickup,\n",
    "                                  MAX( dropoff_datetime) AS max_dropoff,\n",
    "                                  MAX( trip_time_in_secs) AS max_trip_time,\n",
    "                                  MAX( trip_distance) AS max_trip_distance,\n",
    "                                  MAX( total_amount) AS max_total_amount\n",
    "                                  FROM data\"\"\")\n",
    "statistics.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "62d35c1d-ee12-4dba-a04f-0c587b37dbbc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-----\n",
    "## Ploting information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f3458188-7f50-4c67-bac8-d0c2aab3daf7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### First plot\n",
    "\n",
    "This first plot helps showing that data has several invalid values.\n",
    "\n",
    "Let's plot the pickups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d49038ab-9e5f-4e5b-a40f-a8bc65cd5db4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Group project\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "mySchema = StructType([\n",
    "    StructField(\"medallion\", StringType()),\n",
    "    StructField(\"hack_license\", StringType()),\n",
    "    StructField(\"pickup_datetime\", TimestampType()),\n",
    "    StructField(\"dropoff_datetime\", TimestampType()),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType()),\n",
    "    StructField(\"trip_distance\", DoubleType()),\n",
    "    StructField(\"pickup_longitude\", DoubleType()),\n",
    "    StructField(\"pickup_latitude\", DoubleType()),\n",
    "    StructField(\"dropoff_longitude\", DoubleType()),\n",
    "    StructField(\"dropoff_latitude\", DoubleType()),\n",
    "    StructField(\"payment_type\", StringType()),\n",
    "    StructField(\"fare_amount\", DoubleType()),\n",
    "    StructField(\"surcharge\", DoubleType()),\n",
    "    StructField(\"mta_tax\", DoubleType()),\n",
    "    StructField(\"tip_amount\", DoubleType()),\n",
    "    StructField(\"tolls_amount\", DoubleType()),\n",
    "    StructField(\"total_amount\", DoubleType()),\n",
    "])\n",
    "\n",
    "dataset = spark.read.load(FILENAME, format=\"csv\", \n",
    "                         sep=\",\", schema=mySchema, header=\"false\")\n",
    "dataset.createOrReplaceTempView(\"data\")\n",
    "\n",
    "# Plotting all points is probably too much in many computers, so lets plot only a few 1000's\n",
    "data = spark.sql( \"SELECT * FROM data LIMIT 10000\")\n",
    "pickups = data.collect()\n",
    "\n",
    "print('Plotting pickups')\n",
    "plt.scatter([row.pickup_longitude for row in pickups],\n",
    "            [row.pickup_latitude for row in pickups],\n",
    "            s=1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fa71110e-d0bf-4340-8a4b-7b14085175a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Plotting heatamps\n",
    "\n",
    "Plotting **plot heatmaps** and how to **create a grid** with coordinates. We need to group nearby coordinates together or (almost) every coordinate will be unique.\n",
    "\n",
    "The example creates a grid, with cell of 150m of side (commented out the code for cells of 500m of side).\n",
    "\n",
    "\\[From ACM DEBS 2015 Grand Challenge page\\]\n",
    "\n",
    "**Question 1**: Is the earth flat or how to map coordinates to cells?\n",
    "\n",
    "**Answer**: For the challenge we allow a simplified flat earth assumption for mapping coordinates to cells in the queries. You can assume that a distance of 500 meter south corresponds to a change of 0.004491556 degrees in the coordinate system. For moving 500 meter east you can assume a change of 0.005986 degrees in the coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e304fc50-bd5b-46b1-8ad1-6da0d2bb9663",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Group project\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#Squares of 500 meters\n",
    "latitudeStep = 0.004491556\n",
    "longitudeStep = 0.005986\n",
    "northLatitude = 41.474937 - 0.5 * latitudeStep\n",
    "southLatitude = northLatitude - 300 * latitudeStep\n",
    "eastLongitude = -74.913585 - 0.5 * longitudeStep\n",
    "westLongitude = eastLongitude + 300 * longitudeStep\n",
    "\n",
    "# Squares of 150 meters\n",
    "#latitudeStep = 0.0013474668\n",
    "#longitudeStep = 0.0017958\n",
    "#northLatitude = 40.95\n",
    "#southLatitude = northLatitude - 300 * latitudeStep\n",
    "#eastLongitude = -74.2\n",
    "#westLongitude = eastLongitude + 300 * longitudeStep\n",
    "\n",
    "# function to round longitude to a point in the middle of the square\n",
    "def longiRound( val):\n",
    "    return ((val - eastLongitude) // longitudeStep) * longitudeStep + eastLongitude + longitudeStep / 2\n",
    "spark.udf.register(\"longround\", longiRound, DoubleType())\n",
    "\n",
    "# function to round latitude to a point in the middle of the square\n",
    "def latRound( l):\n",
    "    return northLatitude - ((northLatitude - l) // latitudeStep) * latitudeStep - latitudeStep / 2\n",
    "spark.udf.register(\"latround\", latRound, DoubleType())\n",
    "\n",
    "mySchema = StructType([\n",
    "    StructField(\"medallion\", StringType()),\n",
    "    StructField(\"hack_license\", StringType()),\n",
    "    StructField(\"pickup_datetime\", TimestampType()),\n",
    "    StructField(\"dropoff_datetime\", TimestampType()),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType()),\n",
    "    StructField(\"trip_distance\", DoubleType()),\n",
    "    StructField(\"pickup_longitude\", DoubleType()),\n",
    "    StructField(\"pickup_latitude\", DoubleType()),\n",
    "    StructField(\"dropoff_longitude\", DoubleType()),\n",
    "    StructField(\"dropoff_latitude\", DoubleType()),\n",
    "    StructField(\"payment_type\", StringType()),\n",
    "    StructField(\"fare_amount\", DoubleType()),\n",
    "    StructField(\"surcharge\", DoubleType()),\n",
    "    StructField(\"mta_tax\", DoubleType()),\n",
    "    StructField(\"tip_amount\", DoubleType()),\n",
    "    StructField(\"tolls_amount\", DoubleType()),\n",
    "    StructField(\"total_amount\", DoubleType()),\n",
    "])\n",
    "\n",
    "dataset = spark.read.load(FILENAME, format=\"csv\", \n",
    "                         sep=\",\", schema=mySchema, header=\"false\")\n",
    "\n",
    "# Let's filter data outside of the box and build a grid\n",
    "# Points in each square are mapped to the center of the square.\n",
    "dataset.createOrReplaceTempView(\"data\")\n",
    "filteredDataDF = spark.sql( \"\"\"SELECT medallion, hack_license, pickup_datetime,\n",
    "                                    dropoff_datetime, trip_time_in_secs, trip_distance,\n",
    "                                    longround(pickup_longitude) AS pickup_longitude, \n",
    "                                    latround(pickup_latitude) AS pickup_latitude,\n",
    "                                    longround(dropoff_longitude) AS dropoff_longitude, \n",
    "                                    latround(dropoff_latitude) AS dropoff_latitude, \n",
    "                                    payment_type, fare_amount, mta_tax, \n",
    "                                    tip_amount, tolls_amount, total_amount\n",
    "                                  FROM data\n",
    "                                  WHERE pickup_longitude >= \"\"\" + str(eastLongitude) + \"\"\" AND\n",
    "                                  pickup_longitude <=  \"\"\" + str(westLongitude) + \"\"\" AND\n",
    "                                  dropoff_longitude >=  \"\"\" + str(eastLongitude) + \"\"\" AND\n",
    "                                  dropoff_longitude <=  \"\"\" + str(westLongitude) + \"\"\" AND\n",
    "                                  pickup_latitude <= \"\"\" + str(northLatitude) + \"\"\" AND\n",
    "                                  pickup_latitude >= \"\"\" + str(southLatitude) + \"\"\" AND\n",
    "                                  dropoff_latitude <=  \"\"\" + str(northLatitude) + \"\"\" AND\n",
    "                                  dropoff_latitude >=  \"\"\" + str(southLatitude))\n",
    "filteredDataDF.createOrReplaceTempView(\"data\")\n",
    "\n",
    "# Frequency for pickups\n",
    "pickupsDF = spark.sql( \"\"\"SELECT pickup_longitude, pickup_latitude, count(*) AS cnt\n",
    "                                  FROM data\n",
    "                                  GROUP BY pickup_longitude, pickup_latitude\"\"\")\n",
    "pickups = pickupsDF.collect()\n",
    "\n",
    "print('Plotting pickups')\n",
    "p = plt.scatter([row.pickup_longitude for row in pickups],\n",
    "            [row.pickup_latitude for row in pickups],\n",
    "            c=[row.cnt for row in pickups],s=1,cmap=\"rainbow\")\n",
    "plt.colorbar(p)\n",
    "plt.show()\n",
    "\n",
    "# Statistics for pickups\n",
    "dropoffsDF = spark.sql( \"\"\"SELECT dropoff_longitude, dropoff_latitude, count(*) AS cnt\n",
    "                                  FROM data\n",
    "                                  GROUP BY dropoff_longitude, dropoff_latitude\"\"\")\n",
    "dropoffs = dropoffsDF.collect()\n",
    "\n",
    "print('Plotting pickups')\n",
    "p = plt.scatter([row.dropoff_longitude for row in dropoffs],\n",
    "            [row.dropoff_latitude for row in dropoffs],\n",
    "            c=[row.cnt for row in dropoffs],s=1,cmap=\"rainbow\")\n",
    "plt.colorbar(p)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d9eb532f-aee3-4732-8db8-9b627a785c47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exercise 0: another simple statistics\n",
    "\n",
    "This example computes, for each license, the number of trips performed.\n",
    "\n",
    "We have the code using Spark and Pandas, printing the time for doing the computation.\n",
    "**Draw some conclusions** by comparing the time for performing the computation using Spark and Pandas, and also when using the small and long dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9e667272-66f4-4789-9136-69fe489293b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Code: Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aa3a555d-41ce-4301-bb60-9c13bacb2b0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Group project\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time = time.time()\n",
    "mySchema = StructType([\n",
    "    StructField(\"medallion\", StringType()),\n",
    "    StructField(\"hack_license\", StringType()),\n",
    "    StructField(\"pickup_datetime\", TimestampType()),\n",
    "    StructField(\"dropoff_datetime\", TimestampType()),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType()),\n",
    "    StructField(\"trip_distance\", DoubleType()),\n",
    "    StructField(\"pickup_longitude\", DoubleType()),\n",
    "    StructField(\"pickup_latitude\", DoubleType()),\n",
    "    StructField(\"dropoff_longitude\", DoubleType()),\n",
    "    StructField(\"dropoff_latitude\", DoubleType()),\n",
    "    StructField(\"payment_type\", StringType()),\n",
    "    StructField(\"fare_amount\", DoubleType()),\n",
    "    StructField(\"surcharge\", DoubleType()),\n",
    "    StructField(\"mta_tax\", DoubleType()),\n",
    "    StructField(\"tip_amount\", DoubleType()),\n",
    "    StructField(\"tolls_amount\", DoubleType()),\n",
    "    StructField(\"total_amount\", DoubleType()),\n",
    "])\n",
    "\n",
    "dataset = spark.read.load(FILENAME, format=\"csv\", \n",
    "                         sep=\",\", schema=mySchema, header=\"false\")\n",
    "dataset.createOrReplaceTempView(\"data\")\n",
    "statistics = spark.sql( \"\"\"SELECT hack_license, COUNT(*) AS cnt FROM data GROUP BY hack_license\"\"\")\n",
    "statistics.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print( \"Runtime = \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cea0ff85-3d10-4a6a-b24a-4cc66b780f3c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Results (Spark)\n",
    "\n",
    "**In our computer:**\n",
    "\n",
    "The time to process the small dataset was : **TO COMPLETE** seconds. 10.244155168533325\n",
    "\n",
    "The time to process the large dataset was : **TO COMPLETE** seconds. 830.9792747497559\n",
    "\n",
    "**In Azure Databricks:**\n",
    "\n",
    "The time to process the small dataset was : **TO COMPLETE** seconds. 8.084648132324219\n",
    "\n",
    "The time to process the large dataset was : **TO COMPLETE** seconds. 525.8180425167084"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fbceb57e-91d7-430a-8406-ad44dec1b38a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Code: Pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2708ac4e-177f-4c8b-a7c3-b9ebe08a55f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "mySchema = [\"medallion\", \"hack_license\", \"pickup_datetime\",\n",
    "            \"dropoff_datetime\", \"trip_time_in_secs\", \"trip_distance\",\n",
    "            \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\",\n",
    "            \"dropoff_latitude\", \"payment_type\", \"fare_amount\", \n",
    "            \"surcharge\", \"mta_tax\", \"tip_amount\",\n",
    "            \"tolls_amount\", \"total_amount\"]\n",
    "\n",
    "dataset = pd.read_csv(FILENAME,names=mySchema)\n",
    "result = dataset.groupby(\"hack_license\").count()\n",
    "print(result)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print( \"Runtime = \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d81bf814-b05b-4e88-9d6b-b31cbff8a580",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Results (Pandas)\n",
    "\n",
    "This will not work in Databricks.\n",
    "\n",
    "**In our computer:**\n",
    "\n",
    "The time to process the small dataset was : **44.3519287109375** seconds. \n",
    "\n",
    "The time to process the large dataset was : **(could not run)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ae84b317-68ee-4eec-96c7-f010f52b2757",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Results discussion\n",
    "\n",
    "Sample vs. All data\n",
    "\n",
    "The sample dataset takes a lot less time than the larger dataset, when run both locally and on Azure Databricks. This result was expected due to their difference in size: the small datset comprises data of taxi trips of 20 days (roughly 2 million events; 130 MB) while the large dataset has data fot the whole year of 2013 (roughly 173 million events; 33GB). Particularly, in Pandas, it was not possible to run the large dataset locally.\n",
    "\n",
    "Pandas vs. Spark (incomplete)\n",
    "\n",
    "When running the small dataset locally, Pandas takes a lot more time than Spark (44.35 secs vs. 10.24).\n",
    "The large dataset was not possible to run on pandas.\n",
    "\n",
    "Spark locally vs. Spark Azure\n",
    "\n",
    "When comparing running the code on Spark locally and in Azure Databricks, running both the sample dataset and the large dataset is much faster in Azure Databricks (8.085 secs. and 525.818 secs.) than locally (10.244 secs and 525.818 secs). because Azure Databricks is a cloud-based data engineering tool that  is made to process large sets of data, by storing the files in different servers, and therefore allowing simultaneous processes in multiple machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step for this project was to clean our dataset.\n",
    "#### 1. New York City Limits\n",
    "First, we only want consider coordinates inside of the New York City area. We also want to eliminate coordinates that might have been misrecorded (like latitudes equal to 0).\n",
    "For this purpose, we considered a degree of 41 latitude for the most Northern Point of NYC, and and 40.4 for the Southern limit. For the longitude, we considered and limit of - 73.6 for the most eastern boundary and -74.5 for the most western boundary.\n",
    "#### 2. Trip Distance\n",
    "Secondly, we want to eliminate any trips that had their distance potentially wrongly recorded, and get rid of outliers. For this, we only considered trips below 25 miles.\n",
    "\n",
    "#### 3. Trip Time\n",
    "Then we eliminated every trip with the time marked as 0 seconds, as these were likely wrong entries and/or are not to be considered. We also eliminate trips that were above 1 hour length, and these are considered outliers when looking at the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean = spark.sql(\"SELECT * FROM data \\\n",
    "                    WHERE pickup_longitude BETWEEN -74.5 AND -73.6 AND pickup_latitude BETWEEN 40.4 AND 41\\\n",
    "                    AND dropoff_longitude BETWEEN -74.5 AND -73.6 AND dropoff_latitude BETWEEN 40.4 AND 41\\\n",
    "                    AND trip_time_in_secs != 0\\\n",
    "                    AND trip_distance < 25\\\n",
    "                    AND trip_time_in_secs < 3600\")\n",
    "clean.createOrReplaceTempView(\"clean_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b5345a02-01de-46da-aaeb-150747d13ae4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "Let's start by trying to help the city to identify which new express bus routes shoud introduce. To this end, you should find the most frequent routes whose distance is above a given treshold (defined by you).\n",
    "\n",
    "For establishing these routes, we suggest that you use a grid of 500m of side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Squares of 500 meters\n",
    "latitudeStep = 0.004491556\n",
    "longitudeStep = 0.005986\n",
    "northLatitude = 41.474937 - 0.5 * latitudeStep\n",
    "southLatitude = northLatitude - 300 * latitudeStep\n",
    "eastLongitude = -74.913585 - 0.5 * longitudeStep\n",
    "westLongitude = eastLongitude + 300 * longitudeStep\n",
    "\n",
    "def longiRound( val):\n",
    "    return ((val - westLongitude) // longitudeStep) * longitudeStep + \\\n",
    "                westLongitude + longitudeStep / 2\n",
    "\n",
    "spark.udf.register(\"longround\", longiRound, DoubleType())\n",
    "\n",
    "def latRound( l):\n",
    "    return northLatitude - ((northLatitude - l) // latitudeStep) * \\\n",
    "        latitudeStep - latitudeStep / 2\n",
    "spark.udf.register(\"latround\", latRound, DoubleType())\n",
    "\n",
    "top_routes = spark.sql(\"\"\"SELECT latround(pickup_latitude) as p1_lat, longround(pickup_longitude) as p1_long,\\\n",
    "                    latround(dropoff_latitude) as p2_lat, longround(dropoff_longitude) as p2_long,\\\n",
    "                    COUNT(*) as Metric\\\n",
    "                    FROM clean_data WHERE trip_distance > 2 \\\n",
    "                    GROUP BY p1_lat, p1_long, p2_lat, p2_long \\\n",
    "                    ORDER BY metric DESC\"\"\" )\n",
    "top_routes.createOrReplaceTempView(\"top_routes\")\n",
    "top_routes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "303faa61-b5b1-4ac9-a08e-905b9e4c5708",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Discussion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we considered that it was only worth it to build a new bus route for walking distances between 30 to 40 minutes or above, that is, we assume that individuals may prefer to walk in distances below 40 or 30 minutes. With this logic, we only considered taxi trips above 2 miles.\n",
    "\n",
    "In a bus with more stops, shorter walks would be considered, but given that this an express bus with just 1 source and 1 destination, we consider slightly longer distance. In the end, all of our top 8 routes were above 3 miles in distance).\n",
    "\n",
    "We used a grid of 500m of side to join points that are not sinificantly far away from each other.\n",
    "Group By allows us to have the frequency of trips (our Metric) for each combinations of source and destination. We ordered our results by the ones with the highest Metric value to show the most relevant routes first.\n",
    "\n",
    "The most common taxi route was from the Terminal area of the LaGardia airport (40.77425426399999, -73.872021) and the area around the intersection of East 49th Street with Park Avenue, in midtown Manhattan (40.756288039999994, -73.973783) (https://goo.gl/maps/X19MU7sMDsWogVDRA). This should be our new bus route.\n",
    "\n",
    "If we want to build more bus routes, and looking strictly at the pick up and drop off points to make one-destination busroutes, the following are the 7 next to be considered (from most important to less important, as shown in the dataframe):\n",
    "- https://goo.gl/maps/HWhoiyRwTWTo6rgCA (from Midtwon Manhattan to La Guardia Airport)\n",
    "- https://goo.gl/maps/iQPYpaJQSsMq29Gx7 (from La Guardia Airport to Midtwon Manhattan)\n",
    "- https://goo.gl/maps/Yxh1VPD3RXGTbJsu8 (from La Guardia Airport to Midtwon Manhattan)\n",
    "- https://goo.gl/maps/Hp2TE6N9cT8GWvWb8 (from La Guardia Airport to Midtwon Manhattan)\n",
    "- https://goo.gl/maps/AWdeaNnvzS3W7JkGA (from La Guardia Airport to Midtwon Manhattan)\n",
    "- https://goo.gl/maps/P8JaoC3sNDNjwVS29 (from Midtown Manhattan to La Guardia Airport)\n",
    "- https://goo.gl/maps/V7dmBGK6osHGpKeYA (from Midtown Manhattan to the Upper East side)\n",
    "- https://goo.gl/maps/ocVaU7VyPE9tciha6 (from Midtown Manhattan to La Guardia Airport)\n",
    "- https://goo.gl/maps/qX5G7P3BZYx2fhSB9 (from La Guardia Airport to Midtwon Manhattan)\n",
    "\n",
    "However, if we wanted to combine the 10 routes above, we could plan the following:\n",
    "- From the 6 routes above that go from the airport to Midtown Manhatten, one bus from Lagardia Airport that stops in those 6 different areas of Midtown Manhattan\n",
    "- From the 3 routes that go from Midtown Manhattan to the La Guardia Airport, have a bus that passes through those 3 Midtown Manhattan areas and goes to the airport.\n",
    "- Have a 3rd express bus as the one above, from Midtown Manhattan to the Upper East side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "127cd86a-9a70-4d72-88c3-33f4880359c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "The second question intends to help taxi drivers to decide to which area of the city they should go next. To this end, we could have a web site/mobile app where the drivers could check the best area at a given moment. \n",
    "To support such application efficiently, it would be necessary to have a pre-computed index with the value for each area and period of time (e.g. combining the week day and a period of one hour). \n",
    "\n",
    "You should create the program to create such index. The output tuples should be something like: longitude, latitude, day_of_week, hour value.\n",
    "\n",
    "Define your own metric for the value of an area. Parameters that may be included in such metric include: the number of pickups in the area, the amount collected in the trip, the average time a taxi is idle in the area, etc.\n",
    "\n",
    "Besides presenting the code, explain the rationale of your solution.\n",
    "\n",
    "**Note:** SQL functions date(col), dayofweek(col) and hour(col) return, respectively, the date, day of week and hour of a datatime in column col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hello, taxi driver! Thank you for visiting our app. Our algorithm will help you decide where you should go next.')\n",
    "\n",
    "print('\\nGetting our data...')\n",
    "#gives 3 extra fields: date, day of the week, and hour\n",
    "data_time = spark.sql(\"SELECT *, date(pickup_datetime) as Date, dayofweek(pickup_datetime) as Week_Day, \\\n",
    "                        hour(pickup_datetime) as Hour from clean_data\")\n",
    "data_time.createOrReplaceTempView(\"datatime\")\n",
    "\n",
    "#transforming longitudes and latitudes\n",
    "code_df=spark.sql(\"\"\"SELECT *, longround(pickup_longitude) as p1_long, \\\n",
    "                        latround(pickup_latitude) as p1_lat FROM datatime\"\"\")\n",
    "code_df.createOrReplaceTempView(\"code_df\")\n",
    "\n",
    "#Getting a dataframe with the relevant values for this problem\n",
    "values_df=spark.sql(\"\"\"SELECT * FROM(SELECT Week_Day, Hour, p1_long, p1_lat, \\\n",
    "                    count(*) as n_trips, sum(total_amount) AS payment,\\\n",
    "                    sum(trip_time_in_secs)\\\n",
    "                    FROM code_df\\\n",
    "                    GROUP BY Week_Day, Hour, p1_long, p1_lat)\\\n",
    "                    WHERE n_trips > 12\"\"\") #firstly we only want to consider areas where there is more than one client every 5 minutes\n",
    "values_df.createOrReplaceTempView(\"values\")\n",
    "\n",
    "#Data with the number of trips, and average revenue per trip, per each pick_up location\n",
    "some_metrics=spark.sql(\"\"\"SELECT Week_Day, Hour, Week_Day ||'_'|| Hour as code, p1_long, p1_lat, n_trips,\\\n",
    "                    payment, payment/n_trips as avgpay\\\n",
    "                    FROM values\"\"\")\n",
    "some_metrics.createOrReplaceTempView(\"some_metrics\")\n",
    "\n",
    "#Getting for each hour, four each day of the week, the number of trips above the 75 percentile. \n",
    "demand_new=spark.sql(\"SELECT *, Week_Day1 ||'_'|| Hour1 as code1 FROM\\\n",
    "                        (SELECT Week_Day as Week_Day1, Hour as Hour1, percentile(n_trips, 0.75) as percentile_75 FROM some_metrics\\\n",
    "                        GROUP BY Week_Day, Hour\\\n",
    "                        ORDER BY Week_Day, Hour)\")\n",
    "demand_new.createOrReplaceTempView(\"demand_new\")\n",
    "\n",
    "#Joining the two dataframes, to have a columnn with the Percentile 75 of n_trips\n",
    "join_demand=spark.sql(\"SELECT * FROM some_metrics JOIN demand_new ON some_metrics.code=demand_new.code1\").drop('code').drop('Week_Day1','Hour1')\n",
    "join_demand.createOrReplaceTempView(\"join_demand\")\n",
    "\n",
    "#Eliminating, for each hour for each day of the week, the pick up locations where the number of trips is below the percentile 75\n",
    "selected_demand=spark.sql(\"SELECT *, Week_Day ||'_'|| Hour as code FROM\\\n",
    "                            (SELECT Week_Day, Hour, p1_long, p1_lat, n_trips, avgpay FROM join_demand\\\n",
    "                            WHERE n_trips > percentile_75)\")\n",
    "selected_demand.createOrReplaceTempView(\"selected_demand\")\n",
    "\n",
    "print('Here are several good locations you can go to. Rank is given by the average revenue per trip. Please wait a moment.')\n",
    "#Ranking, for each hour for each day of the week, the selected locations, by their average revenue per trip\n",
    "locations_ranked = spark.sql(\"\"\" SELECT * FROM \\\n",
    "                                (SELECT Week_Day, Hour, MAX(avgpay) as metric, p1_lat,  p1_long,\\\n",
    "                                row_number() over (partition by Week_Day, Hour ORDER BY MAX(avgpay) desc) as max_rank_metric\\\n",
    "                                FROM selected_demand\n",
    "                                WHERE n_trips >= 5 \n",
    "                                GROUP BY Week_Day, Hour, p1_lat,  p1_long\n",
    "                                ORDER BY Week_Day, Hour, max_rank_metric ASC)\"\"\")\n",
    "\n",
    "locations_ranked.show(30)\n",
    "locations_ranked.createOrReplaceTempView(\"locations_ranked\")\n",
    "\n",
    "print('If you prefer, here is THE BEST location for each hour of the day, for each day of the week.')\n",
    "the_best_location = spark.sql(\"\"\" SELECT * FROM \\\n",
    "                                (SELECT Week_Day, Hour, MAX(avgpay) as metric, p1_lat,  p1_long,\\\n",
    "                                row_number() over (partition by Week_Day, Hour ORDER BY MAX(avgpay) desc) as max_rank_metric\\\n",
    "                                FROM selected_demand\n",
    "                                WHERE n_trips >= 5 \n",
    "                                GROUP BY Week_Day, Hour, p1_lat,  p1_long\n",
    "                                ORDER BY Week_Day, Hour)\\\n",
    "                                WHERE max_rank_metric = 1\"\"\")\n",
    "\n",
    "the_best_location.show(30)\n",
    "the_best_location.createOrReplaceTempView(\"the_best_location\")\n",
    "\n",
    "print('Our algorithm is done. We hope it was helpful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9e0d1790-d4c2-4fbf-a7d7-aa1bea276782",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm rationale:**\n",
    "\n",
    "Firstly, because we wanted our algorithm to recommend areas where the driver wouldn't have to wait more than 5 minutes, we started by eliminating the areas where the number of trips per hour was below or equal to 12, that is, we only considered areas where there is more than 1 client every 5 minutes.\n",
    "\n",
    "Secondly, our algorithm eliminates pick up areas where the number of trips (for every hour, for every day of the week) is below the percentile 75 (of number of trips for each hour, for each day of the week).\n",
    "\n",
    "Lastly our algorithm ranks the selected areas according to the average revenue per trip. The user of the app is able to see the entire rank of the selected trips for each hour, for each day of the week, and also a table with the one best area for each hour, for each day of the week.\n",
    "\n",
    "\n",
    "**Code explanation:**\n",
    "\n",
    "The first two statements, \"data_time\", and \"code_df\", transform the pickup_datetime column into a week day column and hour column that we can use. \n",
    "\n",
    "The third statement, \"values_df\", returns the number of trips, the sum of all payments of those trips, for each other, for every hour, for every day of the week, and takes into consideration only the pick_up areas where the waiting time is, on average, below 5 minutes (be selecting areas where, for every hour, the number of trips is above 12). \n",
    "\n",
    "This concludes the first part of the algorithm.\n",
    "\n",
    "The next statement, \"some_metrics\", adds a columns with the average payment per trip, for every pick up area, for every hour and day of the week, and a code (made of the day of the week and the hour).\n",
    "\n",
    "The \"demand_new\" statement creates a dataframe with the percentile 75 of the number of trips for each hour and day of the week, and also a code (made of the day of the week and the hour).\n",
    "\n",
    "After this, we join the two dataframes described above with the \"join_demand\" statement, and then eliminate the areas where the number of trips is below the percentile 75, with the \"selected_demand\" statement. \n",
    "\n",
    "Thus concluding the second part of our algorithm.\n",
    "\n",
    "The \"locations_ranked\" statement returns all of the selected pick up spots, ranked by the respective average revenue per trip. Lastly, the \"the_best_location\" returns the one unique best location in NYC, for each hour and day of the week.\n",
    "\n",
    "This concludes the third part of our algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "85a2968b-96fa-4772-8e58-f6cdbe580f77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exercise 3\n",
    "\n",
    "The third question intends to define the location of taxi ranks (the places where taxis stop waiting for collecting clients) in a way that tries to minimize the distance a client needs to travel to reach the taxi rank.\n",
    "\n",
    "Consider that you want to establish, at least, 100 taxi ranks - present the code that defines the number and locations of the ranks.\n",
    "\n",
    "**Note:** This dataset is for NYC taxis. So, pickups outside of the city are infrequent and not representative of the demand in such areas. As such, you should focus on pickups in a square that includes NYC (it is ok if the square includes parts outside of the city). Use, for example, the following square:\n",
    "```\n",
    "northLatitude = 40.86\n",
    "southLatitude = 40.68\n",
    "eastLongitude = -74.03\n",
    "westLongitude = -73.92\n",
    "```\n",
    "\n",
    "**Suggestion:** Plot your results as a heatmap, with the color being a measurement of the value of the taxi rank; use the visual feedback to enhance your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import sklearn\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "#create DF with pickups limited to NYC\n",
    "pickupsDF = spark.sql( \"\"\"SELECT x, y, cnt\n",
    "                        FROM(SELECT pickup_longitude as x , pickup_latitude as y, count(*) AS cnt\n",
    "                                  FROM clean_data\n",
    "                                  WHERE pickup_latitude<40.87 AND  pickup_latitude>40.68 AND\n",
    "                                  pickup_longitude<-73.92 AND pickup_longitude > -74.03\n",
    "                                  GROUP BY x, y)\n",
    "                                  \"\"\")\n",
    "\n",
    "#create vector with coordinates of pickup and dropoff\n",
    "assembler = VectorAssembler(inputCols=['x','y'],outputCol=\"features\")\n",
    "assembler.transform(pickupsDF)\n",
    "dataset = assembler.transform(pickupsDF)\n",
    "\n",
    "#find 1centroid for each cluster (create 100 clusters)\n",
    "kmeans = KMeans().setK(100).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "centers = model.clusterCenters() \n",
    "\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "       print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = pd.DataFrame(centers)\n",
    "\n",
    "centersdf = spark.createDataFrame(ce,schema=[\"lon\", \"lata\"])\n",
    "\n",
    "centroids = centersdf.select(\"*\").withColumn(\"ida\", monotonically_increasing_id())\n",
    "w = Window.orderBy('ida')\n",
    "centroids2=centroids.withColumn('index',row_number().over(w)-1)\n",
    "centroids2=centroids2.drop('ida')\n",
    "centroids2.createOrReplaceTempView(\"centroidsdf\")\n",
    "\n",
    "#apply the model to our dataset to find the cluster each point belongs to\n",
    "predictions = model.transform(dataset)\n",
    "predictions.createOrReplaceTempView(\"predi\")\n",
    "\n",
    "predictions_coord = spark.sql(\"\"\"SELECT * FROM predi JOIN centroidsdf ON predi.prediction=centroidsdf.index\"\"\")\n",
    "predictions_coord=predictions_coord.drop('index')\n",
    "predictions_coord.createOrReplaceTempView(\"pred_coord\")\n",
    "predictions_coord.show()\n",
    "\n",
    "#Frequency for each cluster\n",
    "cluster_count = spark.sql(\"\"\"SELECT prediction, lon, lata, count(*) as counter \\\n",
    "                            FROM pred_coord GROUP BY lon, lata, prediction\"\"\")\n",
    "\n",
    "cluster_count.createOrReplaceTempView(\"clust_count\")\n",
    "\n",
    "rank_center = spark.sql(\"\"\"SELECT *, ROW_NUMBER() OVER( ORDER BY counter) Taxi_Rank\\\n",
    "                            FROM clust_count\"\"\")\n",
    "\n",
    "rank_center.show()\n",
    "\n",
    "rank = rank_center.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot center of each 100 cluster\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.figure(figsize=(10,6))\n",
    "plot_cluster = plt.scatter([row.lon for row in rank],\n",
    "            [row.lata for row in rank],\n",
    "           c=[row.Taxi_Rank for row in rank],s=3,cmap=\"rainbow\")\n",
    "plt.colorbar(plot_cluster)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bccdda5f-23d5-4f86-ad4b-d7ac0da223e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Discussion\n",
    "\n",
    "We start by limiting our window of action to NYC limits to ignore trips that might be outside of that area.\n",
    "Afterwards, we use kmeans to create 100 clusters - where the center of each cluster will be the location of the taxi rank. Kmeans allows to group points in such a way that ensures that each point will be allocated to a group where its distance to the center of that group will be lower to the center of any other group. In this context, by creating 100 points with kmeans, we ensure that they are located in places that will allow clients to walk as minimum distance possible.\n",
    "\n",
    "The \"centroids2\" df, gives us the coordinates of each centroid (which is numbered form 0 to 99).\n",
    "\n",
    "In \"predictions_coord\" we have the cluster each pickup point belongs to, as well as the coordinates of the center of the cluster.\n",
    "\n",
    "\"rank\" orders the taxi stops by the number of pickups that exist in the corresponding cluster. The #1 rank is the one with the lowest number of pickups. We plot the centers of each cluster with a color for each rank - a warmer color means that the rank number is closer to 100, which means the corresponding cluster has a higher number of trips. We can clearly see that the clusters with a higher number of pickups are located in the center.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9f55859b-dc21-4d4e-8e8c-94db48b43b94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exercise 4\n",
    "\n",
    "Renova is a portuguese company that sells paper-derived products, such as toilet paper, paper towels, etc. \n",
    "The company plans to enter the market in NYC with an aggressive marketing strategy: it wants to reach as many people as possible by identifying communities where activity is tightly connected, and placing one billboard in each community, at its busiest location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install GraphFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid for close locations\n",
    "\n",
    "#Squares of 150 meters\n",
    "latitudeStep = 0.0013474668\n",
    "longitudeStep = 0.0017958\n",
    "northLatitude = 40.95\n",
    "southLatitude = northLatitude - 300 * latitudeStep\n",
    "eastLongitude = -74.2\n",
    "westLongitude = eastLongitude + 300 * longitudeStep\n",
    "\n",
    "\n",
    "def longiRound( val):\n",
    "    return ((val - westLongitude) // longitudeStep) * longitudeStep + \\\n",
    "                westLongitude + longitudeStep / 2\n",
    "\n",
    "spark.udf.register(\"longround\", longiRound, DoubleType())\n",
    "\n",
    "def latRound( l):\n",
    "    return northLatitude - ((northLatitude - l) // latitudeStep) * \\\n",
    "        latitudeStep - latitudeStep / 2\n",
    "spark.udf.register(\"latround\", latRound, DoubleType())\n",
    "\n",
    "#only consider trips with a distance lower than 1 mile\n",
    "\n",
    "filter_distance = spark.sql(\"\"\"SELECT * FROM clean_data WHERE trip_distance < 1\"\"\")\n",
    "filter_distance.createOrReplaceTempView(\"filtered_dist\")\n",
    "\n",
    "#only consider trips in NYC area \n",
    "\n",
    "filter_NYC = spark.sql(\"\"\"SELECT * FROM(SELECT longround(pickup_longitude) as p1_long, latround(pickup_latitude) as p1_lat,\\\n",
    "                   longround(dropoff_longitude) as p2_long,latround(dropoff_latitude) as p2_lat,\n",
    "                   COUNT(*) as ntripsnyc FROM filtered_dist\\\n",
    "                  GROUP BY p1_long, p1_lat, p2_long, p2_lat \\\n",
    "                   ORDER BY ntripsnyc DESC)\n",
    "                   WHERE p1_lat<40.87 AND p2_lat<40.87 AND p1_lat>40.68 AND p2_lat>40.68\n",
    "                                 AND p1_long<-73.92 AND p2_long<-73.92 AND p1_long > -74.03\n",
    "                                 AND p2_long > -74.03\"\"\" ) \n",
    "filter_NYC.createOrReplaceTempView(\"filtered_NYC\")\n",
    "\n",
    "#exclude locations that have a residual amount of trips (3)\n",
    "filter_ntrips = spark.sql(\"\"\"SELECT * FROM filtered_NYC WHERE ntripsnyc>3\"\"\")\n",
    "filter_ntrips.createOrReplaceTempView(\"relevant_locations\")\n",
    "\n",
    "#define source (pickup coords) and destination (drop off coords)\n",
    "preprocDF = spark.sql( \"\"\"SELECT concat('(',p1_long,',',p1_lat,')') AS src, concat('(',p2_long,',',p2_lat,')') AS dst,\n",
    "p1_long, p1_lat,\n",
    "p2_long, p2_lat FROM relevant_locations\"\"\")\n",
    "preprocDF.createOrReplaceTempView(\"part1\")\n",
    "\n",
    "#define relation between source and destination\n",
    "edges = spark.sql( \"\"\"SELECT src, dst, 'trip' FROM part1\"\"\")\n",
    "\n",
    "#create vertex with coords of relevant points\n",
    "vertex = spark.sql( \"\"\"SELECT src as id, p1_long as longitude, p1_lat as latitude FROM part1\n",
    "UNION\n",
    "SELECT dst as id, p2_long as longitude,\n",
    "p2_lat as latitude  FROM part1\"\"\")\n",
    "\n",
    "#define graphframe\n",
    "g = GraphFrame(vertex, edges)\n",
    "\n",
    "#find communities from network algorithm \n",
    "result = g.labelPropagation(maxIter=5)\n",
    "\n",
    "result.createOrReplaceTempView(\"communities\")\n",
    "\n",
    "#will only consider as a community if it has at least 50 points inside\n",
    "count_filter = spark.sql(\"\"\"SELECT * FROM\n",
    "                            (SELECT label as label1, count(*) as freq FROM communities GROUP BY label1)\n",
    "                            WHERE freq >50 \"\"\")\n",
    "\n",
    "\n",
    "count_filter.createOrReplaceTempView(\"relevant_communities\")\n",
    "\n",
    "points_communities = spark.sql(\"\"\"SELECT * FROM communities JOIN relevant_communities\n",
    "                                ON communities.label = relevant_communities.label1 \"\"\")\n",
    "\n",
    "\n",
    "points_communities = points_communities.drop('label1')\n",
    "points_communities = points_communities.drop('freq')\n",
    "\n",
    "communities_final = points_communities.collect()\n",
    "\n",
    "points_communities.createOrReplaceTempView(\"communitiesdf\")\n",
    "\n",
    "#shows the community each point belongs to and its id\n",
    "points_communities.show()\n",
    "\n",
    "#how many communities exist\n",
    "count_clusters = spark.sql(\"\"\" SELECT COUNT (DISTINCT label) FROM communitiesdf; \"\"\")\n",
    "\n",
    "count_clusters.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dfs with common codes for drop offs, pick ups and communities\n",
    "code_communities= spark.sql( \"\"\"SELECT longitude, latitude, label, longitude ||'_'|| latitude as code \n",
    "FROM communitiesdf\"\"\")\n",
    "\n",
    "code_communities.createOrReplaceTempView(\"c_comm\")\n",
    "code_pickup = spark.sql( \"\"\"SELECT p1_long, p1_lat, p1_long ||'_'|| p1_lat as codep,\n",
    "count(*) as count_p FROM relevant_locations group by p1_lat, p1_long\"\"\")\n",
    "code_dropoff = spark.sql( \"\"\"SELECT p2_long, p2_lat, p2_long ||'_'|| p2_lat as coded,\n",
    "count(*) as count_d FROM relevant_locations group by p2_lat, p2_long, coded\"\"\")\n",
    "\n",
    "code_pickup.createOrReplaceTempView(\"c_pickup\")\n",
    "code_dropoff.createOrReplaceTempView(\"c_dropoff\")\n",
    "\n",
    "pickup_dropoff= spark.sql(\"\"\"SELECT * FROM c_pickup JOIN c_dropoff ON c_pickup.codep=c_dropoff.coded\"\"\")\n",
    "\n",
    "pickup_dropoff.createOrReplaceTempView(\"pick_drop\")\n",
    "\n",
    "#count the number of times someone has passed by each location (wheter in a pick up or a drop off)\n",
    "\n",
    "count_all = spark.sql(\"\"\"SELECT coded, count_p + count_d as count2 FROM pick_drop \"\"\")\n",
    "count_all.createOrReplaceTempView(\"pick_drop_count\")\n",
    "\n",
    "pickdrop_community = spark.sql(\"\"\"SELECT * FROM pick_drop_count JOIN c_comm\n",
    "ON pick_drop_count.coded=c_comm.code\"\"\")\n",
    "\n",
    "pickdrop_community.createOrReplaceTempView(\"pdc\")\n",
    "\n",
    "\n",
    "#select the location with highest frequency for each community\n",
    "top_locations= spark.sql(\"\"\"SELECT * FROM\n",
    "(SELECT *, row_number() over (partition by label order by count2 desc) as coord_rank FROM pdc\n",
    "WHERE count2>5) WHERE coord_rank <2\"\"\")\n",
    "\n",
    "top_locations = top_locations.drop('coded')\n",
    "top_locations = top_locations.drop('count2')\n",
    "\n",
    "\n",
    "top_locations.show()\n",
    "\n",
    "top_loc = top_locations.collect()\n",
    "\n",
    "p = plt.scatter([row.longitude for row in top_loc],\n",
    "            [row.latitude for row in top_loc],\n",
    "            marker=\"+\")\n",
    "\n",
    "print('Plotting pickups')\n",
    "p = plt.scatter([row.longitude for row in communities_final],\n",
    "            [row.latitude for row in communities_final],\n",
    "            c=[row.label for row in communities_final],s=1,cmap=\"rainbow\")\n",
    "plt.colorbar(p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "17ec8715-2b36-4d29-95ab-249b24d221e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Discussion\n",
    "\n",
    "We will consider that the best locations for ADA's billboards will be the ones that are most popular. \n",
    "For this, we should condier various locations that allow to 1) reach the highest number of different people and 2) guarantee a high number of visualizations.\n",
    "\n",
    "We will start by dividing the city in different communities. Each community is defined by people with similar patterns in travelling. By allocating 1 billboard per community, we ensure that we are reaching all of NYC's public.\n",
    "\n",
    "\n",
    "For this, firstly, we use the grid of 150m of side to join points that are not sinificantly far away from each other that they should be considered 2 different locations - a shorter distance is used because we want to find patterns inside the same community.\n",
    "\n",
    "We filter our data to only include coordinates inside the area of NYC. We also exclude the coordinates of points that are below a threshold of what we consider to be the minimum number of trips for the location to be relevant : 3. After testing with various numbers, we found that 3 was the minimum amount to better detect communities.\n",
    "\n",
    "We fount the labelPropagation algorithm from graphFrames be the most appropriate one to find the communities, as it looks for networking relations, meaning that it will join together all points that are connected by similar movements. This is, people that pass by a location in the community will likely pass by another.\n",
    "\n",
    "In order to be considered a community, we find that the cluster must include at least 50 points. Otherwise, it is not enough to be representative of a pattern. This threshhold was also found trough trial and error.\n",
    "\n",
    "In total, 6 clusters were found, meaning the company should create 6 billboards.\n",
    "\n",
    "\n",
    "Finally, for each community, we find the coordinates of the point with most affluency by counting the number of occurences (both pickups and dropoffs) at each pair of longitude and latitude. The top 1 found for each community will be the location of each billboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "Group project resolution - template",
   "notebookOrigID": 800740550193319,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
